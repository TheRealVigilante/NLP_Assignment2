{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\themi\\anaconda3\\envs\\dl_3.10\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay lets see i want to go to a thai restaurant   with less than ten dollars per person',\n",
       " 'i like to eat at lunch time  so that would be eleven a__m to one p__m',\n",
       " 'i dont want to walk for more than five minutes',\n",
       " 'tell me more about the  na nakapan  restaurant on martin luther king',\n",
       " 'i like to go to a hamburger restaurant']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "file_path = \"transcript.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_sentences = []\n",
    "for line in lines:\n",
    "    # Remove identifier (first word of each line)\n",
    "    line = re.sub(r\"^\\d+_\\d+_\\d+\\s+\", \"\", line)\n",
    "    \n",
    "    # Remove weird characters\n",
    "    line = re.sub(r\"\\[.*?\\]|\\<.*?\\>\", \"\", line)\n",
    "\n",
    "    # Append cleaned sentence\n",
    "    cleaned_sentences.append(line.strip().lower())\n",
    "\n",
    "# remove the full stops\n",
    "cleaned_sentences = [re.sub(r'[^\\w\\s]','',sentence) for sentence in cleaned_sentences]\n",
    "\n",
    "cleaned_sentences[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation and Counting Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Uni-gram Counts: [('i', 2817), ('to', 2716), ('like', 1528), ('food', 1252), ('about', 1157)]\n",
      "Sample Bi-gram Counts: [(('like', 'to'), 1186), (('i', 'want'), 911), (('to', 'eat'), 762), (('i', 'would'), 741), (('would', 'like'), 715)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all sentences\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in cleaned_sentences]\n",
    "\n",
    "# Flatten the list of tokens\n",
    "tokens = [word for sentence in tokens for word in sentence]\n",
    "\n",
    "# Compute unigram frequencies\n",
    "unigram_counts = Counter(tokens)\n",
    "\n",
    "# Compute bigram frequencies\n",
    "bigrams = list(nltk.bigrams(tokens))\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Display sample counts\n",
    "print(\"Sample Uni-gram Counts:\", unigram_counts.most_common(5))\n",
    "print(\"Sample Bi-gram Counts:\", bigram_counts.most_common(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-gram FOR REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(lets | okay) = 0.1193\n",
      "P(see | lets) = 0.0473\n",
      "P(i | see) = 0.0588\n",
      "P(want | i) = 0.3234\n",
      "P(to | want) = 0.6586\n"
     ]
    }
   ],
   "source": [
    "# Formula P(w_n | w_{n-1}) = Count(w_{n-1}, w_n) / Count(w_{n-1})\n",
    "\n",
    "bigram_probabilities = {}\n",
    "\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_probabilities[(w1, w2)] = count / unigram_counts[w1]\n",
    "\n",
    "for bigram, prob in list(bigram_probabilities.items())[:5]:\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram probabilities saved to 'bigram_probabilities.csv'.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    [(w1, w2, prob) for (w1, w2), prob in bigram_probabilities.items()],\n",
    "    columns=[\"First_Word\", \"Second_Word\", \"Probability\"]\n",
    ")\n",
    "\n",
    "df.to_csv(\"bigram_probabilities.csv\", index=False)\n",
    "\n",
    "print(\"Bi-gram probabilities saved to 'bigram_probabilities.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_sentence_probability(sentence, bigram_probabilities):\n",
    "    tokens = tokenizer.tokenize(sentence.lower())\n",
    "    \n",
    "    # Generate tokens\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    \n",
    "    bigram_probs = []\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        if bigram in bigram_probabilities:\n",
    "            bigram_probs.append(bigram_probabilities[bigram])\n",
    "        else:\n",
    "            # just in case any bigram is missing\n",
    "            bigram_probs.append(1e-6)\n",
    "    \n",
    "    sentence_probability = np.prod(bigram_probs)\n",
    "    \n",
    "    return sentence_probability, bigram_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(show me all the Arabic food restaurants) = 1.3310537523662384e-11\n",
      "P(I am learning mathematics) = 9.939652112176074e-15\n"
     ]
    }
   ],
   "source": [
    "sentence_1 = \"show me all the Arabic food restaurants\"\n",
    "sentence_2 = \"I am learning mathematics\"\n",
    "\n",
    "prob_1, bigram_probs_1 = calculate_sentence_probability(sentence_1, bigram_probabilities)\n",
    "prob_2, bigram_probs_2 = calculate_sentence_probability(sentence_2, bigram_probabilities)\n",
    "\n",
    "print(f\"P({sentence_1}) = {prob_1}\")\n",
    "print(f\"P({sentence_2}) = {prob_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
